{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial - Implementing a custom analysis block in Lightwood\n",
    "\n",
    "\n",
    "## Introduction\n",
    "\n",
    "As you might already know, Lightwood is designed to be a flexible machine learning (ML) library that is able to abstract and automate the entire ML pipeline. Crucially, it is also designed to be extended or modified very easily according to your needs, essentially offering the entire spectrum between fully automated AutoML and a lightweight wrapper for customized ML pipelines.\n",
    "\n",
    "As such, we can identify several different customizable \"phases\" in the process. The relevant phase for this tutorial is the \"analysis\" that comes after a predictor has been trained. The goal of this phase is to generate useful insights, like accuracy metrics, confusion matrices, feature importance, etc. These particular examples are all included in the core analysis procedure that Lightwood executes.\n",
    "\n",
    "However, the analysis procedure is structured into a sequential execution of \"analysis blocks\". Each analysis block should generate a well-defined set of insights, as well as handling any actions regarding these at inference time.\n",
    "\n",
    "As an example, one of the core blocks is the Inductive Conformal Prediction (`ICP`) block, which handles the confidence estimation of all Lightwood predictors. The logic within can be complex at times, but thanks to the block abstraction we can deal with it in a structured manner. As this `ICP` block is used when generating predictions, it implements the two main methods that the `BaseAnalysisBlock` class specifies: `.analyze()` to setup everything that is needed, and `.explain()` to actually estimate the confidence in any given prediction.\n",
    "\n",
    "\n",
    "## Objective\n",
    "\n",
    "In this tutorial, we will go through the steps required to implement your own analysis blocks to customize the insights of any Lightwood predictor!\n",
    "\n",
    "In particular, we will implement a \"model correlation heatmap\" block: we want to compare the predictions of all mixers inside a `BestOf` ensemble object, to understand how they might differ in their overall behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-03T21:29:56.978393Z",
     "iopub.status.busy": "2022-02-03T21:29:56.977362Z",
     "iopub.status.idle": "2022-02-03T21:29:58.457474Z",
     "shell.execute_reply": "2022-02-03T21:29:58.457729Z"
    }
   },
   "outputs": [],
   "source": [
    "from typing import Dict, Tuple\n",
    "import pandas as pd\n",
    "import lightwood\n",
    "lightwood.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: figuring out what we need\n",
    "\n",
    "When designing an analysis block, an important choice needs to be made: will this block operate when calling the predictor? Or is it only going to describe its performance once in the held-out validation dataset?\n",
    "\n",
    "Being in the former case means we need to implement both `.analyze()` and `.explain()` methods, while the latter case only needs an `.analyze()` method. Our `ModelCorrelationHeatmap` belongs to this second category.\n",
    "\n",
    "Let's start the implementation by inheriting from `BaseAnalysisBlock`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-03T21:29:58.461457Z",
     "iopub.status.busy": "2022-02-03T21:29:58.461199Z",
     "iopub.status.idle": "2022-02-03T21:29:58.585428Z",
     "shell.execute_reply": "2022-02-03T21:29:58.585180Z"
    }
   },
   "outputs": [],
   "source": [
    "from lightwood.analysis import BaseAnalysisBlock\n",
    "\n",
    "class ModelCorrelationHeatmap(BaseAnalysisBlock):\n",
    "    def __init__(self, deps=tuple()):\n",
    "        super().__init__(deps=deps)\n",
    "        \n",
    "    def analyze(self, info: Dict[str, object], **kwargs) -> Dict[str, object]:\n",
    "        return info\n",
    "\n",
    "    def explain(self,\n",
    "                row_insights: pd.DataFrame,\n",
    "                global_insights: Dict[str, object], **kwargs) -> Tuple[pd.DataFrame, Dict[str, object]]:\n",
    "        \n",
    "        return row_insights, global_insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-03T21:29:58.588212Z",
     "iopub.status.busy": "2022-02-03T21:29:58.587939Z",
     "iopub.status.idle": "2022-02-03T21:29:58.589556Z",
     "shell.execute_reply": "2022-02-03T21:29:58.589754Z"
    }
   },
   "outputs": [],
   "source": [
    "ModelCorrelationHeatmap()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right now, our newly created analysis block doesn't do much, apart from returning the `info` and insights (`row_insights` and `global_insights`) exactly as it received them from the previous block.\n",
    "\n",
    "As previously discussed, we only need to implement a procedure that runs post-training, no action is required at inference time. This means we can use the default `.explain()` behavior in the parent class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-03T21:29:58.592434Z",
     "iopub.status.busy": "2022-02-03T21:29:58.592181Z",
     "iopub.status.idle": "2022-02-03T21:29:58.593455Z",
     "shell.execute_reply": "2022-02-03T21:29:58.593652Z"
    }
   },
   "outputs": [],
   "source": [
    "class ModelCorrelationHeatmap(BaseAnalysisBlock):\n",
    "    def __init__(self, deps=tuple()):\n",
    "        super().__init__(deps=deps)\n",
    "        \n",
    "    def analyze(self, info: Dict[str, object], **kwargs) -> Dict[str, object]:\n",
    "        return info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Implementing the custom analysis block\n",
    "\n",
    "Okay, now for the fun bit: we have to implement a correlation heatmap between the predictions of all mixers inside a `BestOf` ensemble. This is currently the only ensemble implemented in Lightwood, but it is a good idea to explicitly check that the type of the ensemble is what we expect.\n",
    "\n",
    "A natural question to ask at this point is: what information do we have to implement the procedure? You'll note that, apart from the `info` dictionary, we receive a `kwargs` dictionary. You can check out the full documentation for more details, but the keys (and respective value types) exposed in this object by default are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-03T21:29:58.596282Z",
     "iopub.status.busy": "2022-02-03T21:29:58.596028Z",
     "iopub.status.idle": "2022-02-03T21:29:58.597193Z",
     "shell.execute_reply": "2022-02-03T21:29:58.597502Z"
    }
   },
   "outputs": [],
   "source": [
    "kwargs = {\n",
    "        'predictor': 'lightwood.ensemble.BaseEnsemble',\n",
    "        'target': 'str',\n",
    "        'input_cols': 'list',\n",
    "        'dtype_dict': 'dict',\n",
    "        'normal_predictions': 'pd.DataFrame',\n",
    "        'data': 'pd.DataFrame',\n",
    "        'train_data': 'lightwood.data.encoded_ds.EncodedDs',\n",
    "        'encoded_val_data': 'lightwood.data.encoded_ds.EncodedDs',\n",
    "        'is_classification': 'bool',\n",
    "        'is_numerical': 'bool',\n",
    "        'is_multi_ts': 'bool',\n",
    "        'stats_info': 'lightwood.api.types.StatisticalAnalysis',\n",
    "        'ts_cfg': 'lightwood.api.types.TimeseriesSettings',\n",
    "        'accuracy_functions': 'list',\n",
    "        'has_pretrained_text_enc': 'bool'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see there is lots to work with, but for this example we will focus on using:\n",
    "\n",
    "1. The `predictor` ensemble\n",
    "2. The `encoded_val_data` to generate predictions for each mixer inside the ensemble\n",
    "\n",
    "And the insight we're want to produce is a matrix that compares the output of all mixers and computes the correlation between them.\n",
    "\n",
    "Let's implement the algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-03T21:29:58.600174Z",
     "iopub.status.busy": "2022-02-03T21:29:58.599887Z",
     "iopub.status.idle": "2022-02-03T21:29:58.601638Z",
     "shell.execute_reply": "2022-02-03T21:29:58.601837Z"
    }
   },
   "outputs": [],
   "source": [
    "%%writefile model_correlation.py\n",
    "\n",
    "from typing import Dict\n",
    "from types import SimpleNamespace\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from lightwood.ensemble import BestOf\n",
    "from lightwood.analysis import BaseAnalysisBlock\n",
    "\n",
    "\n",
    "class ModelCorrelationHeatmap(BaseAnalysisBlock):\n",
    "    def __init__(self, deps=tuple()):\n",
    "        super().__init__(deps=deps)\n",
    "        \n",
    "    def analyze(self, info: Dict[str, object], **kwargs) -> Dict[str, object]:\n",
    "        ns = SimpleNamespace(**kwargs)\n",
    "        \n",
    "        # only triggered with the right type of ensemble\n",
    "        if isinstance(ns.predictor, BestOf):\n",
    "            \n",
    "            # store prediction from every mixer\n",
    "            all_predictions = []\n",
    "\n",
    "            for mixer in ns.predictor.mixers:\n",
    "                predictions = mixer(ns.encoded_val_data)['prediction'].values  # retrieve np.ndarray from the returned pd.DataFrame\n",
    "                all_predictions.append(predictions.flatten().astype(int))  # flatten and cast labels to int\n",
    "\n",
    "            # calculate correlation matrix\n",
    "            corrs = np.corrcoef(np.array(all_predictions))\n",
    "            \n",
    "            # save inside `info` object\n",
    "            info['mixer_correlation'] = corrs\n",
    "        \n",
    "        return info\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the use of `SimpleNamespace` for dot notation accessors.\n",
    "\n",
    "The procedure above is fairly straightforward, as we leverage numpy's `corrcoef()` function to generate the matrix. \n",
    "\n",
    "Finally, it is very important to add the output to `info` so that it is saved inside the actual predictor object. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Exposing the block to Lightwood\n",
    "\n",
    "\n",
    "To use this in an arbitrary script, we need to add the above class (and all necessary imports) to a `.py` file inside one of the following directories:\n",
    "\n",
    "* `~/lightwood_modules` (where `~` is your home directory, e.g. `/Users/username/` for macOS and `/home/username/` for linux\n",
    "* `/etc/lightwood_modules`\n",
    "\n",
    "Lightwood will scan these directories and import any class so that they can be found and used by the `JsonAI` code generating module.\n",
    "\n",
    "**To continue, please save the code cell above as `model_correlation.py` in one of the indicated directories.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Final test run\n",
    "\n",
    "Ok! Everything looks set to try out our custom block. Let's generate a predictor for [this](https://github.com/mindsdb/lightwood/blob/stable/tests/data/hdi.csv) sample dataset, and see whether our new insights are any good.\n",
    "\n",
    "First, it is important to add our `ModelCorrelationHeatmap` to the `analysis_blocks` attribute of the Json AI object that will generate your predictor code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-03T21:29:58.604529Z",
     "iopub.status.busy": "2022-02-03T21:29:58.604262Z",
     "iopub.status.idle": "2022-02-03T21:29:59.210752Z",
     "shell.execute_reply": "2022-02-03T21:29:59.210964Z"
    }
   },
   "outputs": [],
   "source": [
    "from lightwood.api.high_level import ProblemDefinition, json_ai_from_problem, load_custom_module\n",
    "\n",
    "# First, load the custom module we wrote\n",
    "load_custom_module('model_correlation.py')\n",
    "\n",
    "# read dataset\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/mindsdb/lightwood/main/tests/data/hdi.csv')\n",
    "\n",
    "# define the predictive task\n",
    "pdef = ProblemDefinition.from_dict({\n",
    "    'target': 'Development Index',         # column you want to predict\n",
    "    'time_aim': 100,\n",
    "})\n",
    "\n",
    "# generate the Json AI intermediate representation from the data and its corresponding settings\n",
    "json_ai = json_ai_from_problem(df, problem_definition=pdef)\n",
    "\n",
    "# add the custom list of analysis blocks; in this case, composed of a single block\n",
    "json_ai.analysis_blocks = [{\n",
    "    'module': 'model_correlation.ModelCorrelationHeatmap',\n",
    "    'args': {}\n",
    "}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can take a look at the respective Json AI key just to confirm our newly added analysis block is in there:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-03T21:29:59.213815Z",
     "iopub.status.busy": "2022-02-03T21:29:59.213557Z",
     "iopub.status.idle": "2022-02-03T21:29:59.215126Z",
     "shell.execute_reply": "2022-02-03T21:29:59.214910Z"
    }
   },
   "outputs": [],
   "source": [
    "json_ai.analysis_blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to create a predictor from this Json AI, and subsequently train it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-03T21:29:59.218326Z",
     "iopub.status.busy": "2022-02-03T21:29:59.218052Z",
     "iopub.status.idle": "2022-02-03T21:30:04.805303Z",
     "shell.execute_reply": "2022-02-03T21:30:04.805568Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from lightwood.api.high_level import code_from_json_ai, predictor_from_code\n",
    "\n",
    "code = code_from_json_ai(json_ai)\n",
    "predictor = predictor_from_code(code)\n",
    "\n",
    "predictor.learn(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can visualize the mixer correlation matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-03T21:30:04.822276Z",
     "iopub.status.busy": "2022-02-03T21:30:04.821591Z",
     "iopub.status.idle": "2022-02-03T21:30:04.861450Z",
     "shell.execute_reply": "2022-02-03T21:30:04.861243Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "mc = predictor.runtime_analyzer['mixer_correlation']  # newly produced insight\n",
    "\n",
    "mixer_names = [c.__class__.__name__ for c in predictor.ensemble.mixers]\n",
    "\n",
    "# plotting code\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(mc, cmap='seismic')\n",
    "\n",
    "# set ticks\n",
    "ax.set_xticks(np.arange(mc.shape[0]))\n",
    "ax.set_yticks(np.arange(mc.shape[1]))\n",
    "\n",
    "# set tick labels\n",
    "ax.set_xticklabels(mixer_names)\n",
    "ax.set_yticklabels(mixer_names)\n",
    "\n",
    "# show cell values\n",
    "for i in range(len(mixer_names)):\n",
    "    for j in range(len(mixer_names)):\n",
    "        text = ax.text(j, i, round(mc[i, j], 3), ha=\"center\", va=\"center\", color=\"w\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice! We've just added an additional piece of insight regarding the predictor that Lightwood came up with for the task of predicting the Human Development Index of any given country.\n",
    "\n",
    "What this matrix is telling us is whether predictions of each pair of the mixers stored in the ensemble have a high correlation or not.\n",
    "\n",
    "This is, of course, a very simple example, but it shows the convenience of such an abstraction within the broader pipeline that Lightwood automates.\n",
    "\n",
    "For more complex examples, you can check out any of the three core analysis blocks that we use:\n",
    "\n",
    "* `lightwood.analysis.nc.calibrate.ICP`\n",
    "* `lightwood.analysis.helpers.acc_stats.AccStats`\n",
    "* `lightwood.analysis.helpers.feature_importance.PermutationFeatureImportance`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
